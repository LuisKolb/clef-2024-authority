{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. F1 score: 0.7018, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0010', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'True', 'ignore_nei': 'False'}\n",
      "1. F1 score: 0.6942, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0011', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'True', 'ignore_nei': 'True'}\n",
      "2. F1 score: 0.6668, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0000', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'False', 'ignore_nei': 'False'}\n",
      "3. F1 score: 0.6668, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0100', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'True', 'scale': 'False', 'ignore_nei': 'False'}\n",
      "4. F1 score: 0.6608, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'True', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/1010', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'True', 'ignore_nei': 'False'}\n",
      "5. F1 score: 0.6536, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0001', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'False', 'ignore_nei': 'True'}\n",
      "6. F1 score: 0.6536, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'False', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/0101', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'True', 'scale': 'False', 'ignore_nei': 'True'}\n",
      "7. F1 score: 0.6530, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'True', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/1011', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'True', 'ignore_nei': 'True'}\n",
      "8. F1 score: 0.6309, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'True', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/1000', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'False', 'ignore_nei': 'False'}\n",
      "9. F1 score: 0.6309, config: {'blind_run': 'False', 'split': 'combined', 'preprocess': 'True', 'add_author_name': 'False', 'add_author_bio': 'False', 'out_dir': './data-out/experiments/PreNormScaleNei/1001', 'retriever_k': 5, 'retriever_label': 'TERRIER', 'verifier_label': 'LLAMA', 'normalize_scores': 'False', 'scale': 'False', 'ignore_nei': 'True'}\n"
     ]
    }
   ],
   "source": [
    "# find the best config using TERRIER and LLAMA3\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "exp_path = 'data-out/experiments/PreNormScaleNei/'\n",
    "l = []\n",
    "# find al files called log.txt, recursively\n",
    "for root, dirs, files in os.walk(exp_path):\n",
    "    for file in files:\n",
    "        if file.endswith('log.txt'):\n",
    "            # open file, and for each line test if it contains the string \"result for verification run - \"\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                for line in f:\n",
    "                    if 'result for verification run - ' in line:\n",
    "                        f1 = float(line.split('result for verification run - Strict-F1: ')[1].split(' ')[0])\n",
    "                        # now, get string between the brackets { and }\n",
    "                        configs = '{' + line.split('{')[1].split('}')[0] + '}'\n",
    "                        # replace single quotes with double quotes\n",
    "                        configs = configs.replace(\"'\", '\"')\n",
    "                        # convert each boolean value to string\n",
    "                        configs = configs.replace('True', '\"True\"')\n",
    "                        configs = configs.replace('False', '\"False\"')\n",
    "                        \n",
    "                        # convert to json\n",
    "                        config = json.loads(configs)\n",
    "                        l.append((f1, config))\n",
    "\n",
    "# sort the list by the first element, which is the f1 score\n",
    "l.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# print the top 10 results\n",
    "for i, (f1, config) in enumerate(l[:10]):\n",
    "    print(f'{i}. F1 score: {f1:.4f}, config: {config}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 96 training json objects and 32 dev objects\n"
     ]
    }
   ],
   "source": [
    "from utils import load_rumors_from_jsonl\n",
    "import os\n",
    "\n",
    "out_dir = './temp-data'\n",
    "\n",
    "clef_path = '../clef2024-checkthat-lab/task5'\n",
    "data_path = os.path.join(clef_path, 'data')\n",
    "\n",
    "filepath_train = os.path.join(data_path, 'English_train.json')\n",
    "filepath_dev = os.path.join(data_path, 'English_dev.json')\n",
    "\n",
    "train_jsons = load_rumors_from_jsonl(filepath_train)\n",
    "dev_jsons = load_rumors_from_jsonl(filepath_dev)\n",
    "\n",
    "print(f'loaded {len(train_jsons)} training json objects and {len(dev_jsons)} dev objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils import clean_tweet\n",
    "\n",
    "def clean_jsons(jsons):\n",
    "    data_cleaned = []\n",
    "\n",
    "    for entry in jsons:\n",
    "        \n",
    "        tl_clean = []\n",
    "        for account_url, tl_tweet_id, tl_tweet in entry['timeline']:\n",
    "            tl_tweet_cleaned = clean_tweet(tl_tweet)\n",
    "            if tl_tweet_cleaned:\n",
    "                tl_clean += [[account_url, tl_tweet_id, tl_tweet_cleaned]]\n",
    "\n",
    "        ev_clean = []\n",
    "        for account_url, ev_tweet_id, ev_tweet in entry['evidence']:\n",
    "            ev_tweet_cleaned = clean_tweet(ev_tweet)\n",
    "            if ev_tweet_cleaned:\n",
    "                ev_clean += [[account_url, ev_tweet_id, ev_tweet_cleaned]]\n",
    "\n",
    "        data_cleaned += [{\n",
    "            'id': entry['id'],\n",
    "            'rumor': clean_tweet(entry['rumor']),\n",
    "            'label': entry['label'],\n",
    "            'timeline': tl_clean,\n",
    "            'evidence': ev_clean,\n",
    "        }]\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_train = clean_jsons(train_jsons)\n",
    "data_cleaned_dev = clean_jsons(dev_jsons)\n",
    "\n",
    "# data_cleaned_train\n",
    "# data_cleaned_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# if you get the error \"NameError: name '_C' is not defined\" --> restart the Jupyter Kernel\n",
    "\n",
    "def searchPyserini(rumor_id,\n",
    "                   query,\n",
    "                   timeline,\n",
    "                   k = 5,\n",
    "                   temp_dir = 'temp-data-dir',\n",
    "                   index = 'temp-data-dir/index_timeline_dynamic'):\n",
    "    \n",
    "    # ensure \"working directory\" exists (where we store intermediate data like the dynamic index that will be quered later)\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.mkdir(temp_dir)\n",
    "\n",
    "    # set up \"dynamic\" (= temporary) index using timeline data\n",
    "    dynamic_idx_filename = 'eng-train-dynamic.jsonl'\n",
    "    with open(os.path.join(temp_dir, dynamic_idx_filename), mode='w', encoding='utf8') as file:\n",
    "        for tweet in timeline:\n",
    "            id = tweet[1]\n",
    "            text = tweet[2]\n",
    "            file.write(json.dumps({'id': id, 'contents': text}) + '\\n')\n",
    "    \n",
    "    # ensure index directory exists and is empty\n",
    "    if os.path.exists(index):\n",
    "        for filename in os.listdir(index):\n",
    "            if os.path.isfile(os.path.join(index, filename)):\n",
    "                os.remove(os.path.join(index, filename))\n",
    "    else:\n",
    "        os.mkdir(index)\n",
    "\n",
    "    # set up pyserini command since python embeddable is not out yet\n",
    "    nthreads = 1\n",
    "    command = f'python -m pyserini.index.lucene ' \\\n",
    "    f'-input {temp_dir} ' \\\n",
    "    f'-collection JsonCollection ' \\\n",
    "    f'-generator DefaultLuceneDocumentGenerator ' \\\n",
    "    f'-index {index} ' \\\n",
    "    f'-threads {nthreads} ' \\\n",
    "    f'-storePositions ' \\\n",
    "    f'-storeDocvectors ' \\\n",
    "    f'-storeRaw ' \\\n",
    "    f'-language en'\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "\n",
    "    # load searcher from index directoy\n",
    "    searcher = LuceneSearcher(index)\n",
    "    hits = searcher.search(query)\n",
    "\n",
    "    ranked = []\n",
    "\n",
    "    for i, hit in enumerate(hits[:k]):\n",
    "        ranked += [[rumor_id, hit.docid, i+1, hit.score]]\n",
    "\n",
    "        # doc = searcher.doc(hit.docid)\n",
    "        # json_doc = json.loads(doc.raw())\n",
    "        # wrap(f'{i+1:2} {hit.docid:4} {hit.score:.5f}\\n{json_doc[\"contents\"]}')\n",
    "\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AuRED_132', '1590400068208988160', 1, 23.428499221801758],\n",
       " ['AuRED_132', '1591489851106668544', 2, 15.630800247192383],\n",
       " ['AuRED_132', '1589654877890019331', 3, 11.791999816894531],\n",
       " ['AuRED_132', '1589949764107665409', 4, 9.603899955749512],\n",
       " ['AuRED_132', '1591404278996168705', 5, 9.46500015258789]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAS FOUND\t1590400068208988160 After circulating news that the Governor of the Bank of Lebanon Riad Salameh had announced to NBN about raising the value of the dollar and raising the ceiling on banking withdrawals the NBN channel denies the validity of this information that is being circulated citing the channel and confirms that there is no truth to it on this subject\n",
      "NOT FOUND\t1590364198462435329 There is no truth to the information being circulated quoted by the NBN channel regarding a statement by the Governor of the Central Bank regarding banking circulars\n"
     ]
    }
   ],
   "source": [
    "# for testing...\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "query = test_rumor['rumor']\n",
    "timeline = test_rumor['timeline']\n",
    "\n",
    "ranked_docs = searchPyserini(test_rumor['id'], query, timeline)\n",
    "display(ranked_docs)\n",
    "\n",
    "# simple spot check\n",
    "for evidence in test_rumor['evidence']:\n",
    "    print(f'{\"WAS FOUND\" if evidence[1] in [x[1] for x in ranked_docs] else \"NOT FOUND\"}\\t{evidence[1]} {evidence[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6faf2a45f549e4a75c66a719cf2120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in tqdm(data_cleaned_dev):\n",
    "    rumor_id = item['id']\n",
    "    query = item['rumor']\n",
    "    timeline = item['timeline']\n",
    "\n",
    "    data += searchPyserini(rumor_id, query, timeline)\n",
    "\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-dev.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ef83e587d44542a34604eda8950f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in tqdm(data_cleaned_train):\n",
    "    rumor_id = item['id']\n",
    "    query = item['rumor']\n",
    "    timeline = item['timeline']\n",
    "\n",
    "    data += searchPyserini(rumor_id, query, timeline)\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-train.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_relevant_documents_tfidf(rumor_id, query, timeline, k=5):\n",
    "    # Get only doc texts\n",
    "    documents = [t[2] for t in timeline]\n",
    "    tweet_ids = [t[1] for t in timeline]\n",
    "\n",
    "    # Combine query and documents for TF-IDF vectorization\n",
    "    combined_texts = [query] + documents\n",
    "\n",
    "    # Generate TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "    # Calculate similarity of the query to each document\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "    \n",
    "    # Rank documents based on similarity scores\n",
    "    ranked_doc_indices = similarity_scores.argsort()[0][::-1]\n",
    "\n",
    "    ranked = []\n",
    "    for i, idx in enumerate(ranked_doc_indices[:k]):\n",
    "        ranked += [[rumor_id, tweet_ids[idx], i, similarity_scores[0][idx]]]\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "    # # Sort the documents according to rank\n",
    "    # ranked_documents = [documents[i] for i in ranked_doc_indices]\n",
    "    # ranked_scores = [similarity_scores[0][i] for i in ranked_doc_indices]\n",
    "    # ranked_ids = [tweet_ids[i] for i in ranked_doc_indices]\n",
    "\n",
    "    # # Create a list of tuples of shape (doc, score)\n",
    "    # ranked_tuples = (list(zip(ranked_ids, ranked_scores, ranked_documents)))\n",
    "    \n",
    "    # return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 139.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in tqdm(data_cleaned_dev):\n",
    "    rumor_id = item['id']\n",
    "    query = item['rumor']\n",
    "    timeline = item['timeline']\n",
    "    \n",
    "    # ranked_docs = retrieve_relevant_documents(rumor_id, query, timeline)\n",
    "    data += retrieve_relevant_documents_tfidf(rumor_id, query, timeline)\n",
    "\n",
    "    # for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "    #     data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/tfidf-trec-dev.txt'\n",
    "write_trec_format_output(out_path, data, 'TFIDF-BASIC')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def jsons_to_pandas(jsons):\n",
    "    data = []\n",
    "    for entry in jsons:\n",
    "        rumor_id = entry['id']\n",
    "        query = entry['rumor']\n",
    "        timeline = entry['timeline']\n",
    "\n",
    "        for author, tw_id, tw in timeline:\n",
    "            data += [\n",
    "                [rumor_id, \"\".join([x if x.isalnum() else \" \" for x in query]), tw_id, tw]\n",
    "            ]\n",
    "\n",
    "    df = pd.DataFrame(data,\n",
    "                      columns=[\"qid\", \"query\", \"docno\", \"text\"],)\n",
    "    return df\n",
    "\n",
    "df = jsons_to_pandas(data_cleaned_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "VM is already running, can't set classpath/options; VM started at  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n    app.start()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n    result = self._run_cell(\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n    result = runner(coro)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\luisk\\AppData\\Local\\Temp\\ipykernel_11796\\1516532247.py\", line 4, in <module>\n    from pyterrier.batchretrieve import TextScorer\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\pyterrier\\batchretrieve.py\", line 1, in <module>\n    from jnius import autoclass, cast\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius\\__init__.py\", line 36, in <module>\n    from .reflect import *  # noqa\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius\\reflect.py\", line 19, in <module>\n    class Class(JavaClass, metaclass=MetaJavaClass):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatchretrieve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextScorer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pt\u001b[38;5;241m.\u001b[39mstarted():\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m textscorer \u001b[38;5;241m=\u001b[39m TextScorer(takes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m, returns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m, body_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, wmodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m, controls\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqemodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBo1\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     10\u001b[0m rtr \u001b[38;5;241m=\u001b[39m textscorer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[1;32mc:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\pyterrier\\__init__.py:122\u001b[0m, in \u001b[0;36minit\u001b[1;34m(version, mem, packages, jvm_opts, redirect_io, logging, home_dir, boot_packages, tqdm, no_download, helper_version)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjnius_config\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jar \u001b[38;5;129;01min\u001b[39;00m classpathTrJars:\n\u001b[1;32m--> 122\u001b[0m     \u001b[43mjnius_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_classpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_opts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m jvm_opts:\n",
      "File \u001b[1;32mc:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius_config.py:57\u001b[0m, in \u001b[0;36madd_classpath\u001b[1;34m(*path)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_classpath\u001b[39m(\u001b[38;5;241m*\u001b[39mpath):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Appends items to the classpath for the JVM to use.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    Replaces any existing classpath, overriding the CLASSPATH environment variable.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mcheck_vm_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m classpath\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m classpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius_config.py:20\u001b[0m, in \u001b[0;36mcheck_vm_running\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises a ValueError if the VM is already running.\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vm_running:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVM is already running, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt set classpath/options; VM started at\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m vm_started_at)\n",
      "\u001b[1;31mValueError\u001b[0m: VM is already running, can't set classpath/options; VM started at  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n    app.start()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n    result = self._run_cell(\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n    result = runner(coro)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\luisk\\AppData\\Local\\Temp\\ipykernel_11796\\1516532247.py\", line 4, in <module>\n    from pyterrier.batchretrieve import TextScorer\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\pyterrier\\batchretrieve.py\", line 1, in <module>\n    from jnius import autoclass, cast\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius\\__init__.py\", line 36, in <module>\n    from .reflect import *  # noqa\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"c:\\Users\\luisk\\miniconda3\\envs\\clef\\lib\\site-packages\\jnius\\reflect.py\", line 19, in <module>\n    class Class(JavaClass, metaclass=MetaJavaClass):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "    \n",
    "textscorer = TextScorer(takes=\"docs\", returns=\"queries\", body_attr=\"text\", wmodel=\"BM25\", controls={\"qe\":\"on\", \"qemodel\":\"Bo1\"})\n",
    "rtr = textscorer.transform(df)\n",
    "rtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.6859649122807018, 'AP': 0.6412280701754386}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-bm25-qe.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-bm25-qe.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6810818713450292}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-bm25-qe.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-bm25-qe.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6806608187134503}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "# ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-c.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-c.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trec_format_output_from_pandas(filename: str, data, tag: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes data to a file in the TREC format.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The name of the file to write to.\n",
    "    - data (List[Tuple[str, int, int, float]]): A list of tuples, where each tuple contains:\n",
    "        - rumor_id (str): The unique ID for the given rumor.\n",
    "        - authority_tweet_id (int): The unique ID for the authority tweet.\n",
    "        - rank (int): The rank of the authority tweet ID for that given rumor_id.\n",
    "        - score (float): The score given by the model for the authority tweet ID.\n",
    "    - tag (str): The string identifier of the team/model.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for row in range(len(data)):\n",
    "            i = row%5\n",
    "            line = f\"{data.at[i, 'qid']}\\tQ0\\t{data.at[i, 'docno']}\\t{data.at[i, 'rank']}\\t{data.at[i, 'score']}\\t{tag}\\n\"\n",
    "            file.write(line)\n",
    "\n",
    "write_trec_format_output_from_pandas('temp-data/terrier-trec.txt', rtr.query('rank < 5'), 'TERRIER-BM25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6806608187134503}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(rtr.query('rank < 5'), golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>AuRED_099</td>\n",
       "      <td>749</td>\n",
       "      <td>1233784722238705670</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qatar threw Iranian peas into garbage for fear...</td>\n",
       "      <td>Hello my dear brother thank you for your obser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid  docid                docno  rank  score  \\\n",
       "820  AuRED_099    749  1233784722238705670   149    0.0   \n",
       "\n",
       "                                                 query  \\\n",
       "820  Qatar threw Iranian peas into garbage for fear...   \n",
       "\n",
       "                                                  text  \n",
       "820  Hello my dear brother thank you for your obser...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.merge(rtr, df[['docno', 'text']], on='docno', how='left')\n",
    "d[d['text'].str.contains(\"Please note that food items unfit for human consumption are destroyed after they are confiscated\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_relevant_documents_sbert(rumor_id, query, timeline, k=5):\n",
    "    corpus = [t[2] for t in timeline]\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    top_k = min(k, len(corpus))\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    # if debug:\n",
    "    #     print(\"\\n\\n======================\\n\\n\")\n",
    "    #     print(\"Query:\", query)\n",
    "    #     evidence_ids = [e[1] for e in evidence]\n",
    "\n",
    "    found = []\n",
    "    docs = []\n",
    "\n",
    "    for i, (score, idx) in enumerate(zip(top_results[0], top_results[1])):\n",
    "            id = timeline[idx][1]\n",
    "\n",
    "            # if debug:\n",
    "            #     is_evidence = id in evidence_ids\n",
    "            #     star = \"(*)\" if is_evidence else \"\\t\"\n",
    "            #     print(star, '\\t', \"(Rank: {:.0f})\".format(i+1), \"(Score: {:.4f})\".format(score), corpus[idx])\n",
    "            #     if is_evidence: found += [id]\n",
    "\n",
    "            docs += [[rumor_id, id, i+1, score.item()]]\n",
    "\n",
    "    # if debug:    \n",
    "    #     for _, ev_id, ev_text in evidence:\n",
    "    #         if ev_id not in found:\n",
    "    #                 print('(!) ', ev_text)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in data_cleaned_train[:]:\n",
    "    query = item['rumor']\n",
    "    timeline = item['timeline']\n",
    "    rumor_id = item['id']\n",
    "    data += retrieve_relevant_documents_sbert(rumor_id, query, timeline)\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/sbert-trec-train.txt'\n",
    "write_trec_format_output(out_path, data, 'SBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in data_cleaned_dev[:]:\n",
    "    query = item['rumor']\n",
    "    timeline = item['timeline']\n",
    "    rumor_id = item['id']\n",
    "    data += retrieve_relevant_documents_sbert(rumor_id, query, timeline)\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/sbert-trec-dev.txt'\n",
    "write_trec_format_output(out_path, data, 'SBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample \t {'R@5': 0.6357894736842106, 'AP': 0.5612280701754385}\n",
      "lucence  {'R@5': 0.6971929824561404, 'AP': 0.663766081871345}\n",
      "tfidf \t {'R@5': 0.7235087719298245, 'AP': 0.6301754385964913}\n",
      "terrier  {'R@5': 0.6859649122807018, 'AP': 0.6412280701754386}\n",
      "sbert \t {'R@5': 0.7080701754385965, 'AP': 0.6363508771929824}\n"
     ]
    }
   ],
   "source": [
    "from scoring_utils import eval_run_retrieval\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "sample_submission_file = task5_dir + '/submission_samples/KGAT_zeroShot_evidence_English_dev.txt'\n",
    "lucene_submission_file = 'temp-data/lucene-trec-dev.txt'\n",
    "tfidf_submission_file = 'temp-data/tfidf-trec-dev.txt'\n",
    "terrier_submission_file = 'temp-data/terrier-trec-bm25-qe.txt'\n",
    "sbert_submission_file = 'temp-data/sbert-trec-dev.txt'\n",
    "\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "out_file = 'temp-data/out.csv'\n",
    "\n",
    "print('sample', '\\t',eval_run_retrieval(sample_submission_file,golden_labels_file))\n",
    "print('lucence', '', eval_run_retrieval(lucene_submission_file,golden_labels_file))\n",
    "print('tfidf', '\\t', eval_run_retrieval(tfidf_submission_file,golden_labels_file))\n",
    "print('terrier', '', eval_run_retrieval(terrier_submission_file,golden_labels_file))\n",
    "print('sbert', '\\t', eval_run_retrieval(sbert_submission_file,golden_labels_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

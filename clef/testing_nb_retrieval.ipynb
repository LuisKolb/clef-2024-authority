{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 96 training json objects and 32 dev objects\n"
     ]
    }
   ],
   "source": [
    "from utils import load_rumors_from_jsonl\n",
    "import os\n",
    "\n",
    "out_dir = './temp-data'\n",
    "\n",
    "clef_path = '../clef2024-checkthat-lab/task5'\n",
    "data_path = os.path.join(clef_path, 'data')\n",
    "\n",
    "filepath_train = os.path.join(data_path, 'English_train.json')\n",
    "filepath_dev = os.path.join(data_path, 'English_dev.json')\n",
    "\n",
    "train_jsons = load_rumors_from_jsonl(filepath_train)\n",
    "dev_jsons = load_rumors_from_jsonl(filepath_dev)\n",
    "\n",
    "print(f'loaded {len(train_jsons)} training json objects and {len(dev_jsons)} dev objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all training data, maybe to use for a global index?\n",
    "# # but we need to do \"zero-shot\" retrieval...\n",
    "# all_timeline_tweets = {}\n",
    "# duplicates = 0\n",
    "\n",
    "# for d in train_jsons:\n",
    "#     for author_url, id, text in d['timeline']:\n",
    "#         if id not in all_timeline_tweets:\n",
    "#             all_timeline_tweets[id] = {'author_url': author_url, 'text': text }\n",
    "#         else:\n",
    "#             duplicates += 1\n",
    "\n",
    "# print(len(all_timeline_tweets.keys()))\n",
    "# print(duplicates)\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('temp-data/eng-train.jsonl', mode='w') as file:\n",
    "#     file.write('')\n",
    "\n",
    "# with open('temp-data/eng-train.jsonl', mode='a', encoding='utf8') as file:\n",
    "#     for id in all_timeline_tweets:\n",
    "#         o = {'id': id, 'contents': all_timeline_tweets[id]['text']}\n",
    "#         file.write(json.dumps(o) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils import clean_tweet\n",
    "\n",
    "data_cleaned_train = []\n",
    "\n",
    "for entry in train_jsons:\n",
    "    \n",
    "    tl_clean = []\n",
    "    for account_url, tl_tweet_id, tl_tweet in entry['timeline']:\n",
    "        tl_tweet_cleaned = clean_tweet(tl_tweet)\n",
    "        if tl_tweet_cleaned:\n",
    "            tl_clean += [[account_url, tl_tweet_id, tl_tweet_cleaned]]\n",
    "\n",
    "    ev_clean = []\n",
    "    for account_url, ev_tweet_id, ev_tweet in entry['evidence']:\n",
    "        ev_tweet_cleaned = clean_tweet(ev_tweet)\n",
    "        if ev_tweet_cleaned:\n",
    "            ev_clean += [[account_url, ev_tweet_id, ev_tweet_cleaned]]\n",
    "\n",
    "    data_cleaned_train += [{\n",
    "        'id': entry['id'],\n",
    "        'rumor': clean_tweet(entry['rumor']),\n",
    "        'label': entry['label'],\n",
    "        'timeline': tl_clean,\n",
    "        'evidence': ev_clean,\n",
    "    }]\n",
    "\n",
    "# data_cleaned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils import clean_tweet\n",
    "\n",
    "data_cleaned_dev = []\n",
    "\n",
    "for entry in dev_jsons:\n",
    "    \n",
    "    tl_clean = []\n",
    "    for account_url, tl_tweet_id, tl_tweet in entry['timeline']:\n",
    "        tl_tweet_cleaned = clean_tweet(tl_tweet)\n",
    "        if tl_tweet_cleaned:\n",
    "            tl_clean += [[account_url, tl_tweet_id, tl_tweet_cleaned]]\n",
    "\n",
    "    ev_clean = []\n",
    "    for account_url, ev_tweet_id, ev_tweet in entry['evidence']:\n",
    "        ev_tweet_cleaned = clean_tweet(ev_tweet)\n",
    "        if ev_tweet_cleaned:\n",
    "            ev_clean += [[account_url, ev_tweet_id, ev_tweet_cleaned]]\n",
    "\n",
    "    data_cleaned_dev += [{\n",
    "        'id': entry['id'],\n",
    "        'rumor': clean_tweet(entry['rumor']),\n",
    "        'label': entry['label'],\n",
    "        'timeline': tl_clean,\n",
    "        'evidence': ev_clean,\n",
    "    }]\n",
    "\n",
    "# data_cleaned_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# if you get the error \"NameError: name '_C' is not defined\" --> restart the Jupyter Kernel\n",
    "\n",
    "def searchPyserini(query,\n",
    "                   timeline,\n",
    "                   k = 5,\n",
    "                   temp_dir = 'temp-data-dir',\n",
    "                   index = 'temp-data-dir/index_timeline_dynamic'):\n",
    "    \n",
    "    # ensure \"working directory\" exists (where we store intermediate data like the dynamic index that will be quered later)\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.mkdir(temp_dir)\n",
    "\n",
    "    # set up \"dynamic\" (= temporary) index using timeline data\n",
    "    dynamic_idx_filename = 'eng-train-dynamic.jsonl'\n",
    "    with open(os.path.join(temp_dir, dynamic_idx_filename), mode='w', encoding='utf8') as file:\n",
    "        for tweet in timeline:\n",
    "            id = tweet[1]\n",
    "            text = tweet[2]\n",
    "            file.write(json.dumps({'id': id, 'contents': text}) + '\\n')\n",
    "    \n",
    "    # ensure index directory exists and is empty\n",
    "    if os.path.exists(index):\n",
    "        for filename in os.listdir(index):\n",
    "            if os.path.isfile(os.path.join(index, filename)):\n",
    "                os.remove(os.path.join(index, filename))\n",
    "    else:\n",
    "        os.mkdir(index)\n",
    "\n",
    "    # set up pyserini command since python embeddable is not out yet\n",
    "    nthreads = 1\n",
    "    command = f'python -m pyserini.index.lucene ' \\\n",
    "    f'-input {temp_dir} ' \\\n",
    "    f'-collection JsonCollection ' \\\n",
    "    f'-generator DefaultLuceneDocumentGenerator ' \\\n",
    "    f'-index {index} ' \\\n",
    "    f'-threads {nthreads} ' \\\n",
    "    f'-storePositions ' \\\n",
    "    f'-storeDocvectors ' \\\n",
    "    f'-storeRaw ' \\\n",
    "    f'-language en'\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "\n",
    "    # load searcher from index directoy\n",
    "    searcher = LuceneSearcher(index)\n",
    "    hits = searcher.search(query)\n",
    "\n",
    "    ranked_tuples = []\n",
    "\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.docid)\n",
    "        json_doc = json.loads(doc.raw())\n",
    "\n",
    "        ranked_tuples += [(hit.docid, hit.score, json_doc[\"contents\"])]\n",
    "\n",
    "        # wrap(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}\\n{json_doc[\"contents\"]}')\n",
    "\n",
    "    return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1590400068208988160',\n",
       "  23.428499221801758,\n",
       "  'After circulating news that the Governor of the Bank of Lebanon Riad Salameh had announced to NBN about raising the value of the dollar and raising the ceiling on banking withdrawals the NBN channel denies the validity of this information that is being circulated citing the channel and confirms that there is no truth to it on this subject'),\n",
       " ('1591489851106668544',\n",
       "  15.630800247192383,\n",
       "  'Raising the exchange rate of the customs dollar the TVA and withdrawals from banks What are its repercussions and results Is there economic and financial stability in light of the current political chaos Report Rasha Al-Zein Hashem'),\n",
       " ('1589654877890019331',\n",
       "  11.791999816894531,\n",
       "  'The exchange rate of the dollar rose on the black market as it touched the threshold of 39 000 liras recording 38 900 liras per dollar'),\n",
       " ('1589949764107665409',\n",
       "  9.603899955749512,\n",
       "  \"Turkish Minister of Energy Turkey's purchases of natural gas from Russia have begun to be partially paid in Russian rubles The share of payments in local currency in exchange for energy imports from Russia will increase in the coming months\"),\n",
       " ('1591404278996168705',\n",
       "  9.46500015258789,\n",
       "  '“Regie” continues to receive tobacco crops from farmers in dollars'),\n",
       " ('1589952497829171203',\n",
       "  9.430899620056152,\n",
       "  '“The Ministry of Industry issued a statement setting the ceiling for the selling price of a ton of black soil (factory gate) at three million and forty thousand Lebanese pounds ” “provided that this price is effective from tomorrow Wednesday 11 9 2022 until Tuesday 11 15 2022 inclusive ” No The price mentioned above includes value added tax'),\n",
       " ('1589552194109706240',\n",
       "  9.324399948120117,\n",
       "  'United Nations 15 8 million people in Sudan need aid next year'),\n",
       " ('1589921293683789824',\n",
       "  9.097700119018555,\n",
       "  '6 000 Argentine fans were prevented from entering the World Cup stadiums'),\n",
       " ('1589910543183548417',\n",
       "  9.092599868774414,\n",
       "  'The occupation army arrests 8 Palestinians from various areas in the West Bank'),\n",
       " ('1589672879981199360',\n",
       "  9.046299934387207,\n",
       "  'The Bank of Lebanon announced that “the trading volume on the Sayrafa platform for today amounted to 25 million US dollars at a rate of 30 100 Lebanese pounds per dollar according to the exchange rates of operations carried out by banks and exchange institutions on the platform ” \"')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAS FOUND\t1590400068208988160 After circulating news that the Governor of the Bank of Lebanon Riad Salameh had announced to NBN about raising the value of the dollar and raising the ceiling on banking withdrawals the NBN channel denies the validity of this information that is being circulated citing the channel and confirms that there is no truth to it on this subject\n",
      "NOT FOUND\t1590364198462435329 There is no truth to the information being circulated quoted by the NBN channel regarding a statement by the Governor of the Central Bank regarding banking circulars\n"
     ]
    }
   ],
   "source": [
    "# for testing...\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "query = test_rumor['rumor']\n",
    "timeline = test_rumor['timeline']\n",
    "\n",
    "ranked_docs = searchPyserini(query, timeline)\n",
    "display(ranked_docs)\n",
    "\n",
    "# simple spot check\n",
    "for evidence in test_rumor['evidence']:\n",
    "    print(f'{\"WAS FOUND\" if evidence[1] in [x[0] for x in ranked_docs] else \"NOT FOUND\"}\\t{evidence[1]} {evidence[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:12<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_dev):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    ranked_docs = searchPyserini(query, timeline)\n",
    "\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-dev.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:36<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_train):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    ranked_docs = searchPyserini(query, timeline)\n",
    "\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-train.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_relevant_documents(query, timeline):\n",
    "    # Get only doc texts\n",
    "    documents = [t[2] for t in timeline]\n",
    "    tweet_ids = [t[1] for t in timeline]\n",
    "\n",
    "    # Combine query and documents for TF-IDF vectorization\n",
    "    combined_texts = [query] + documents\n",
    "    print(combined_texts)\n",
    "    # Generate TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "    # Calculate similarity of the query to each document\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "    print(similarity_scores)\n",
    "    \n",
    "    # Rank documents based on similarity scores\n",
    "    ranked_doc_indices = similarity_scores.argsort()[0][::-1]\n",
    "\n",
    "    # Sort the documents according to rank\n",
    "    ranked_documents = [documents[i] for i in ranked_doc_indices]\n",
    "    ranked_scores = [similarity_scores[0][i] for i in ranked_doc_indices]\n",
    "    ranked_ids = [tweet_ids[i] for i in ranked_doc_indices]\n",
    "\n",
    "    # Create a list of tuples of shape (doc, score)\n",
    "    ranked_tuples = (list(zip(ranked_ids, ranked_scores, ranked_documents)))\n",
    "    \n",
    "    return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 134.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_dev):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    # for t in timeline:\n",
    "    #     print('\\t', t)\n",
    "    \n",
    "    ranked_docs = retrieve_relevant_documents(query, timeline)\n",
    "    \n",
    "    # try:\n",
    "    # except IndexError:\n",
    "        # print(query)\n",
    "        # for t in timeline:\n",
    "        #     print('\\t', t)\n",
    "        # pass\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/tfidf-trec.txt'\n",
    "write_trec_format_output(out_path, data, 'TFIDF-BASIC')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anserini 0.22.0 fatjar not found, downloading to C:\\Users\\luisk\\.pyterrier...\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample \t {'R@5': 0.6357894736842106, 'AP': 0.5612280701754385}\n",
      "lucence  {'R@5': 0.6971929824561404, 'AP': 0.663766081871345}\n",
      "tfidf \t {'R@5': 0.7235087719298245, 'AP': 0.6301754385964913}\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "def evaluate_run(pred_path,golden_path):\n",
    "    golden = ptio.read_qrels(golden_path)\n",
    "    pred= ptio._read_results_trec(pred_path)\n",
    "    eval= ptpipelines.Evaluate(pred, golden , metrics = [R@5,MAP],perquery=False)\n",
    "    return eval\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "sample_submission_file = task5_dir + '/submission_samples/KGAT_zeroShot_evidence_English_dev.txt'\n",
    "lucene_submission_file = 'temp-data/lucene-trec.txt'\n",
    "tfidf_submission_file = 'temp-data/tfidf-trec.txt'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "out_file = 'temp-data/out.csv'\n",
    "\n",
    "print('sample', '\\t', evaluate_run(sample_submission_file,golden_labels_file))\n",
    "print('lucence', '', evaluate_run(lucene_submission_file,golden_labels_file))\n",
    "print('tfidf', '\\t', evaluate_run(tfidf_submission_file,golden_labels_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

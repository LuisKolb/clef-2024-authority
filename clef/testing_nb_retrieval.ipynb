{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 96 training json objects and 32 dev objects\n"
     ]
    }
   ],
   "source": [
    "from utils import load_rumors_from_jsonl\n",
    "import os\n",
    "\n",
    "out_dir = './temp-data'\n",
    "\n",
    "clef_path = '../clef2024-checkthat-lab/task5'\n",
    "data_path = os.path.join(clef_path, 'data')\n",
    "\n",
    "filepath_train = os.path.join(data_path, 'English_train.json')\n",
    "filepath_dev = os.path.join(data_path, 'English_dev.json')\n",
    "\n",
    "train_jsons = load_rumors_from_jsonl(filepath_train)\n",
    "dev_jsons = load_rumors_from_jsonl(filepath_dev)\n",
    "\n",
    "print(f'loaded {len(train_jsons)} training json objects and {len(dev_jsons)} dev objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils import clean_tweet\n",
    "\n",
    "data_cleaned_train = []\n",
    "\n",
    "for entry in train_jsons:\n",
    "    \n",
    "    tl_clean = []\n",
    "    for account_url, tl_tweet_id, tl_tweet in entry['timeline']:\n",
    "        tl_tweet_cleaned = clean_tweet(tl_tweet)\n",
    "        if tl_tweet_cleaned:\n",
    "            tl_clean += [[account_url, tl_tweet_id, tl_tweet_cleaned]]\n",
    "\n",
    "    ev_clean = []\n",
    "    for account_url, ev_tweet_id, ev_tweet in entry['evidence']:\n",
    "        ev_tweet_cleaned = clean_tweet(ev_tweet)\n",
    "        if ev_tweet_cleaned:\n",
    "            ev_clean += [[account_url, ev_tweet_id, ev_tweet_cleaned]]\n",
    "\n",
    "    data_cleaned_train += [{\n",
    "        'id': entry['id'],\n",
    "        'rumor': clean_tweet(entry['rumor']),\n",
    "        'label': entry['label'],\n",
    "        'timeline': tl_clean,\n",
    "        'evidence': ev_clean,\n",
    "    }]\n",
    "\n",
    "# data_cleaned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils import clean_tweet\n",
    "\n",
    "data_cleaned_dev = []\n",
    "\n",
    "for entry in dev_jsons:\n",
    "    \n",
    "    tl_clean = []\n",
    "    for account_url, tl_tweet_id, tl_tweet in entry['timeline']:\n",
    "        tl_tweet_cleaned = clean_tweet(tl_tweet)\n",
    "        if tl_tweet_cleaned:\n",
    "            tl_clean += [[account_url, tl_tweet_id, tl_tweet_cleaned]]\n",
    "\n",
    "    ev_clean = []\n",
    "    for account_url, ev_tweet_id, ev_tweet in entry['evidence']:\n",
    "        ev_tweet_cleaned = clean_tweet(ev_tweet)\n",
    "        if ev_tweet_cleaned:\n",
    "            ev_clean += [[account_url, ev_tweet_id, ev_tweet_cleaned]]\n",
    "\n",
    "    data_cleaned_dev += [{\n",
    "        'id': entry['id'],\n",
    "        'rumor': clean_tweet(entry['rumor']),\n",
    "        'label': entry['label'],\n",
    "        'timeline': tl_clean,\n",
    "        'evidence': ev_clean,\n",
    "    }]\n",
    "\n",
    "# data_cleaned_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# if you get the error \"NameError: name '_C' is not defined\" --> restart the Jupyter Kernel\n",
    "\n",
    "def searchPyserini(query,\n",
    "                   timeline,\n",
    "                   k = 5,\n",
    "                   temp_dir = 'temp-data-dir',\n",
    "                   index = 'temp-data-dir/index_timeline_dynamic'):\n",
    "    \n",
    "    # ensure \"working directory\" exists (where we store intermediate data like the dynamic index that will be quered later)\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.mkdir(temp_dir)\n",
    "\n",
    "    # set up \"dynamic\" (= temporary) index using timeline data\n",
    "    dynamic_idx_filename = 'eng-train-dynamic.jsonl'\n",
    "    with open(os.path.join(temp_dir, dynamic_idx_filename), mode='w', encoding='utf8') as file:\n",
    "        for tweet in timeline:\n",
    "            id = tweet[1]\n",
    "            text = tweet[2]\n",
    "            file.write(json.dumps({'id': id, 'contents': text}) + '\\n')\n",
    "    \n",
    "    # ensure index directory exists and is empty\n",
    "    if os.path.exists(index):\n",
    "        for filename in os.listdir(index):\n",
    "            if os.path.isfile(os.path.join(index, filename)):\n",
    "                os.remove(os.path.join(index, filename))\n",
    "    else:\n",
    "        os.mkdir(index)\n",
    "\n",
    "    # set up pyserini command since python embeddable is not out yet\n",
    "    nthreads = 1\n",
    "    command = f'python -m pyserini.index.lucene ' \\\n",
    "    f'-input {temp_dir} ' \\\n",
    "    f'-collection JsonCollection ' \\\n",
    "    f'-generator DefaultLuceneDocumentGenerator ' \\\n",
    "    f'-index {index} ' \\\n",
    "    f'-threads {nthreads} ' \\\n",
    "    f'-storePositions ' \\\n",
    "    f'-storeDocvectors ' \\\n",
    "    f'-storeRaw ' \\\n",
    "    f'-language en'\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "\n",
    "    # load searcher from index directoy\n",
    "    searcher = LuceneSearcher(index)\n",
    "    hits = searcher.search(query)\n",
    "\n",
    "    ranked_tuples = []\n",
    "\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.docid)\n",
    "        json_doc = json.loads(doc.raw())\n",
    "\n",
    "        ranked_tuples += [(hit.docid, hit.score, json_doc[\"contents\"])]\n",
    "\n",
    "        # wrap(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}\\n{json_doc[\"contents\"]}')\n",
    "\n",
    "    return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1590400068208988160',\n",
       "  23.428499221801758,\n",
       "  'After circulating news that the Governor of the Bank of Lebanon Riad Salameh had announced to NBN about raising the value of the dollar and raising the ceiling on banking withdrawals the NBN channel denies the validity of this information that is being circulated citing the channel and confirms that there is no truth to it on this subject'),\n",
       " ('1591489851106668544',\n",
       "  15.630800247192383,\n",
       "  'Raising the exchange rate of the customs dollar the TVA and withdrawals from banks What are its repercussions and results Is there economic and financial stability in light of the current political chaos Report Rasha Al-Zein Hashem'),\n",
       " ('1589654877890019331',\n",
       "  11.791999816894531,\n",
       "  'The exchange rate of the dollar rose on the black market as it touched the threshold of 39 000 liras recording 38 900 liras per dollar'),\n",
       " ('1589949764107665409',\n",
       "  9.603899955749512,\n",
       "  \"Turkish Minister of Energy Turkey's purchases of natural gas from Russia have begun to be partially paid in Russian rubles The share of payments in local currency in exchange for energy imports from Russia will increase in the coming months\"),\n",
       " ('1591404278996168705',\n",
       "  9.46500015258789,\n",
       "  '“Regie” continues to receive tobacco crops from farmers in dollars'),\n",
       " ('1589952497829171203',\n",
       "  9.430899620056152,\n",
       "  '“The Ministry of Industry issued a statement setting the ceiling for the selling price of a ton of black soil (factory gate) at three million and forty thousand Lebanese pounds ” “provided that this price is effective from tomorrow Wednesday 11 9 2022 until Tuesday 11 15 2022 inclusive ” No The price mentioned above includes value added tax'),\n",
       " ('1589552194109706240',\n",
       "  9.324399948120117,\n",
       "  'United Nations 15 8 million people in Sudan need aid next year'),\n",
       " ('1589921293683789824',\n",
       "  9.097700119018555,\n",
       "  '6 000 Argentine fans were prevented from entering the World Cup stadiums'),\n",
       " ('1589910543183548417',\n",
       "  9.092599868774414,\n",
       "  'The occupation army arrests 8 Palestinians from various areas in the West Bank'),\n",
       " ('1589672879981199360',\n",
       "  9.046299934387207,\n",
       "  'The Bank of Lebanon announced that “the trading volume on the Sayrafa platform for today amounted to 25 million US dollars at a rate of 30 100 Lebanese pounds per dollar according to the exchange rates of operations carried out by banks and exchange institutions on the platform ” \"')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAS FOUND\t1590400068208988160 After circulating news that the Governor of the Bank of Lebanon Riad Salameh had announced to NBN about raising the value of the dollar and raising the ceiling on banking withdrawals the NBN channel denies the validity of this information that is being circulated citing the channel and confirms that there is no truth to it on this subject\n",
      "NOT FOUND\t1590364198462435329 There is no truth to the information being circulated quoted by the NBN channel regarding a statement by the Governor of the Central Bank regarding banking circulars\n"
     ]
    }
   ],
   "source": [
    "# for testing...\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "test_rumor = data_cleaned_dev[2]\n",
    "query = test_rumor['rumor']\n",
    "timeline = test_rumor['timeline']\n",
    "\n",
    "ranked_docs = searchPyserini(query, timeline)\n",
    "display(ranked_docs)\n",
    "\n",
    "# simple spot check\n",
    "for evidence in test_rumor['evidence']:\n",
    "    print(f'{\"WAS FOUND\" if evidence[1] in [x[0] for x in ranked_docs] else \"NOT FOUND\"}\\t{evidence[1]} {evidence[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:12<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_dev):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    ranked_docs = searchPyserini(query, timeline)\n",
    "\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-dev.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:36<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_train):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    ranked_docs = searchPyserini(query, timeline)\n",
    "\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/lucene-trec-train.txt'\n",
    "write_trec_format_output(out_path, data, 'LUCENE')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_relevant_documents(query, timeline):\n",
    "    # Get only doc texts\n",
    "    documents = [t[2] for t in timeline]\n",
    "    tweet_ids = [t[1] for t in timeline]\n",
    "\n",
    "    # Combine query and documents for TF-IDF vectorization\n",
    "    combined_texts = [query] + documents\n",
    "    print(combined_texts)\n",
    "    # Generate TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "    # Calculate similarity of the query to each document\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "    print(similarity_scores)\n",
    "    \n",
    "    # Rank documents based on similarity scores\n",
    "    ranked_doc_indices = similarity_scores.argsort()[0][::-1]\n",
    "\n",
    "    # Sort the documents according to rank\n",
    "    ranked_documents = [documents[i] for i in ranked_doc_indices]\n",
    "    ranked_scores = [similarity_scores[0][i] for i in ranked_doc_indices]\n",
    "    ranked_ids = [tweet_ids[i] for i in ranked_doc_indices]\n",
    "\n",
    "    # Create a list of tuples of shape (doc, score)\n",
    "    ranked_tuples = (list(zip(ranked_ids, ranked_scores, ranked_documents)))\n",
    "    \n",
    "    return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 134.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for r in tqdm(data_cleaned_dev):\n",
    "    rumor_id = r['id']\n",
    "    query = r['rumor']\n",
    "    timeline = r['timeline']\n",
    "\n",
    "    # for t in timeline:\n",
    "    #     print('\\t', t)\n",
    "    \n",
    "    ranked_docs = retrieve_relevant_documents(query, timeline)\n",
    "    \n",
    "    # try:\n",
    "    # except IndexError:\n",
    "        # print(query)\n",
    "        # for t in timeline:\n",
    "        #     print('\\t', t)\n",
    "        # pass\n",
    "    for rank, (authority_tweet_id, score, doc_text) in enumerate(ranked_docs[:5]):\n",
    "        data += [(rumor_id, authority_tweet_id, rank+1, score)]\n",
    "\n",
    "from utils import write_trec_format_output\n",
    "\n",
    "out_path = 'temp-data/tfidf-trec.txt'\n",
    "write_trec_format_output(out_path, data, 'TFIDF-BASIC')\n",
    "\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def jsons_to_pandas(jsons):\n",
    "    data = []\n",
    "    for entry in jsons:\n",
    "        rumor_id = entry['id']\n",
    "        query = entry['rumor']\n",
    "        timeline = entry['timeline']\n",
    "\n",
    "        for author, tw_id, tw in timeline:\n",
    "            data += [\n",
    "                [rumor_id, \"\".join([x if x.isalnum() else \" \" for x in query]), tw_id, tw]\n",
    "            ]\n",
    "\n",
    "    df = pd.DataFrame(data,\n",
    "                      columns=[\"qid\", \"query\", \"docno\", \"text\"],)\n",
    "    return df\n",
    "\n",
    "df = jsons_to_pandas(data_cleaned_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:39:56.207 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AuRED_142</td>\n",
       "      <td>8</td>\n",
       "      <td>1555424541509386240</td>\n",
       "      <td>0</td>\n",
       "      <td>27.443717</td>\n",
       "      <td>Naturalization decree in preparation Lebanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AuRED_142</td>\n",
       "      <td>3</td>\n",
       "      <td>1555986659279360001</td>\n",
       "      <td>1</td>\n",
       "      <td>24.391958</td>\n",
       "      <td>Naturalization decree in preparation Lebanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AuRED_142</td>\n",
       "      <td>4</td>\n",
       "      <td>1555495801962614786</td>\n",
       "      <td>2</td>\n",
       "      <td>8.471113</td>\n",
       "      <td>Naturalization decree in preparation Lebanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AuRED_142</td>\n",
       "      <td>2</td>\n",
       "      <td>1556558220533157890</td>\n",
       "      <td>3</td>\n",
       "      <td>6.758340</td>\n",
       "      <td>Naturalization decree in preparation Lebanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AuRED_142</td>\n",
       "      <td>13</td>\n",
       "      <td>1554743913197477888</td>\n",
       "      <td>4</td>\n",
       "      <td>6.454263</td>\n",
       "      <td>Naturalization decree in preparation Lebanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>AuRED_003</td>\n",
       "      <td>4562</td>\n",
       "      <td>1224611427606040577</td>\n",
       "      <td>47</td>\n",
       "      <td>2.903710</td>\n",
       "      <td>Under the directives of Haitham bin Tariq Al S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4570</th>\n",
       "      <td>AuRED_003</td>\n",
       "      <td>4525</td>\n",
       "      <td>1225255537669017603</td>\n",
       "      <td>48</td>\n",
       "      <td>2.665409</td>\n",
       "      <td>Under the directives of Haitham bin Tariq Al S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>AuRED_003</td>\n",
       "      <td>4568</td>\n",
       "      <td>1226564900425785344</td>\n",
       "      <td>49</td>\n",
       "      <td>1.213530</td>\n",
       "      <td>Under the directives of Haitham bin Tariq Al S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>AuRED_003</td>\n",
       "      <td>4538</td>\n",
       "      <td>1226469326766723072</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Under the directives of Haitham bin Tariq Al S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>AuRED_003</td>\n",
       "      <td>4572</td>\n",
       "      <td>1226216325095329795</td>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Under the directives of Haitham bin Tariq Al S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4574 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid  docid                docno  rank      score  \\\n",
       "0     AuRED_142      8  1555424541509386240     0  27.443717   \n",
       "1     AuRED_142      3  1555986659279360001     1  24.391958   \n",
       "2     AuRED_142      4  1555495801962614786     2   8.471113   \n",
       "3     AuRED_142      2  1556558220533157890     3   6.758340   \n",
       "4     AuRED_142     13  1554743913197477888     4   6.454263   \n",
       "...         ...    ...                  ...   ...        ...   \n",
       "4569  AuRED_003   4562  1224611427606040577    47   2.903710   \n",
       "4570  AuRED_003   4525  1225255537669017603    48   2.665409   \n",
       "4571  AuRED_003   4568  1226564900425785344    49   1.213530   \n",
       "4572  AuRED_003   4538  1226469326766723072    50   0.000000   \n",
       "4573  AuRED_003   4572  1226216325095329795    51   0.000000   \n",
       "\n",
       "                                                  query  \n",
       "0     Naturalization decree in preparation Lebanese ...  \n",
       "1     Naturalization decree in preparation Lebanese ...  \n",
       "2     Naturalization decree in preparation Lebanese ...  \n",
       "3     Naturalization decree in preparation Lebanese ...  \n",
       "4     Naturalization decree in preparation Lebanese ...  \n",
       "...                                                 ...  \n",
       "4569  Under the directives of Haitham bin Tariq Al S...  \n",
       "4570  Under the directives of Haitham bin Tariq Al S...  \n",
       "4571  Under the directives of Haitham bin Tariq Al S...  \n",
       "4572  Under the directives of Haitham bin Tariq Al S...  \n",
       "4573  Under the directives of Haitham bin Tariq Al S...  \n",
       "\n",
       "[4574 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "    \n",
    "textscorer = TextScorer(takes=\"docs\", returns=\"queries\", body_attr=\"text\", wmodel=\"BM25\", controls={\"qe\":\"on\", \"qemodel\":\"Bo1\"})\n",
    "rtr = textscorer.transform(df)\n",
    "rtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.6859649122807018, 'AP': 0.6412280701754386}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-bm25-qe.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-bm25-qe.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6810818713450292}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-bm25-qe.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-bm25-qe.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6806608187134503}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "# ptio._write_results_trec( rtr.query('rank < 5'), 'temp-data/terrier-trec-c.txt')\n",
    "d = ptio._read_results_trec('temp-data/terrier-trec-c.txt')\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trec_format_output_from_pandas(filename: str, data, tag: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes data to a file in the TREC format.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The name of the file to write to.\n",
    "    - data (List[Tuple[str, int, int, float]]): A list of tuples, where each tuple contains:\n",
    "        - rumor_id (str): The unique ID for the given rumor.\n",
    "        - authority_tweet_id (int): The unique ID for the authority tweet.\n",
    "        - rank (int): The rank of the authority tweet ID for that given rumor_id.\n",
    "        - score (float): The score given by the model for the authority tweet ID.\n",
    "    - tag (str): The string identifier of the team/model.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for row in range(len(data)):\n",
    "            i = row%5\n",
    "            line = f\"{data.at[i, 'qid']}\\tQ0\\t{data.at[i, 'docno']}\\t{data.at[i, 'rank']}\\t{data.at[i, 'score']}\\t{tag}\\n\"\n",
    "            file.write(line)\n",
    "\n",
    "write_trec_format_output_from_pandas('temp-data/terrier-trec.txt', rtr.query('rank < 5'), 'TERRIER-BM25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R@5': 0.7189473684210527, 'AP': 0.6806608187134503}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(rtr.query('rank < 5'), golden , metrics = [R@5,MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>AuRED_099</td>\n",
       "      <td>749</td>\n",
       "      <td>1233784722238705670</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qatar threw Iranian peas into garbage for fear...</td>\n",
       "      <td>Hello my dear brother thank you for your obser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid  docid                docno  rank  score  \\\n",
       "820  AuRED_099    749  1233784722238705670   149    0.0   \n",
       "\n",
       "                                                 query  \\\n",
       "820  Qatar threw Iranian peas into garbage for fear...   \n",
       "\n",
       "                                                  text  \n",
       "820  Hello my dear brother thank you for your obser...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.merge(rtr, df[['docno', 'text']], on='docno', how='left')\n",
    "d[d['text'].str.contains(\"Please note that food items unfit for human consumption are destroyed after they are confiscated\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample \t {'R@5': 0.6357894736842106, 'AP': 0.5612280701754385}\n",
      "lucence  {'R@5': 0.0, 'AP': 0.0}\n",
      "tfidf \t {'R@5': 0.7235087719298245, 'AP': 0.6301754385964913}\n",
      "terrier  {'R@5': 0.05263157894736842, 'AP': 0.05263157894736842}\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "def evaluate_run(pred_path,golden_path):\n",
    "    golden = ptio.read_qrels(golden_path)\n",
    "    pred= ptio._read_results_trec(pred_path)\n",
    "    eval= ptpipelines.Evaluate(pred, golden , metrics = [R@5,MAP],perquery=False)\n",
    "    return eval\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "sample_submission_file = task5_dir + '/submission_samples/KGAT_zeroShot_evidence_English_dev.txt'\n",
    "lucene_submission_file = 'temp-data/lucene-trec.txt'\n",
    "tfidf_submission_file = 'temp-data/tfidf-trec.txt'\n",
    "terrier_submission_file = 'temp-data/terrier-trec.txt'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "out_file = 'temp-data/out.csv'\n",
    "\n",
    "print('sample', '\\t', evaluate_run(sample_submission_file,golden_labels_file))\n",
    "print('lucence', '', evaluate_run(lucene_submission_file,golden_labels_file))\n",
    "print('tfidf', '\\t', evaluate_run(tfidf_submission_file,golden_labels_file))\n",
    "print('terrier', '', evaluate_run(terrier_submission_file,golden_labels_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 96 training json lines and 32 dev json lines.\n"
     ]
    }
   ],
   "source": [
    "from clef.utils.data_loading import load_datasets\n",
    "from clef.utils.data_loading import write_trec_format_output\n",
    "from clef.retrieval.retrieve import retrieve_evidence\n",
    "\n",
    "train, dev = load_datasets(preprocess=True)\n",
    "\n",
    "out_dir = 'data-out/exp-add-author-info'\n",
    "\n",
    "# ensure out_dir directories exist for later\n",
    "import os\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    if not os.path.exists(os.path.join(out_dir, 'eval')):\n",
    "        os.mkdir(os.path.join(out_dir, 'eval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f31fc996ce4025a57b2c8ae46e7126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 153 lines to data-out/exp-preprocessing-off/LUCENE-dev.trec.txt\n"
     ]
    }
   ],
   "source": [
    "method = 'LUCENE'\n",
    "data = retrieve_evidence(dev, method, kwargs={})\n",
    "write_trec_format_output(f'{out_dir}/{method}-dev.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'LUCENE'\n",
    "# data = retrieve_evidence(train, method, kwargs={})\n",
    "# write_trec_format_output(f'{out_dir}/{method}-train.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bce4b4274e4a4888d669fae15233cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 160 lines to data-out/exp-preprocessing-off/TFIDF-dev.trec.txt\n"
     ]
    }
   ],
   "source": [
    "method = 'TFIDF'\n",
    "data = retrieve_evidence(dev, method, kwargs={})\n",
    "write_trec_format_output(f'{out_dir}/{method}-dev.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6d7cd0de7c4fbab26b056364015786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 160 lines to data-out/exp-preprocessing-off/SBERT-dev.trec.txt\n"
     ]
    }
   ],
   "source": [
    "method = 'SBERT'\n",
    "data = retrieve_evidence(dev, method, kwargs={})\n",
    "write_trec_format_output(f'{out_dir}/{method}-dev.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'SBERT'\n",
    "# data = retrieve_evidence(train, method, kwargs={})\n",
    "# write_trec_format_output(f'{out_dir}/{method}-train.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53bd35df47e469e936737b8e2341d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 160 lines to data-out/exp-preprocessing-off/OPENAI-dev.trec.txt\n"
     ]
    }
   ],
   "source": [
    "method = 'OPENAI'\n",
    "data = retrieve_evidence(dev, method, kwargs={})\n",
    "write_trec_format_output(f'{out_dir}/{method}-dev.trec.txt', data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def jsons_to_pandas(jsons):\n",
    "    data = []\n",
    "    for entry in jsons:\n",
    "        rumor_id = entry['id']\n",
    "        query = entry['rumor']\n",
    "        timeline = entry['timeline']\n",
    "\n",
    "        for author, tw_id, tw in timeline:\n",
    "            data += [\n",
    "                [rumor_id, \"\".join([x if x.isalnum() else \" \" for x in query]), tw_id, tw]\n",
    "            ]\n",
    "\n",
    "    df = pd.DataFrame(data,\n",
    "                      columns=[\"qid\", \"query\", \"docno\", \"text\"],)\n",
    "    return df\n",
    "\n",
    "df = jsons_to_pandas(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qe off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:42:16.023 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.2947368421052632,\n",
       " 'R@5': 0.7210526315789473,\n",
       " 'AP': 0.7000000000000001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import P, R, MAP\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "wmodel = 'BM25'\n",
    "\n",
    "textscorer = TextScorer(takes=\"docs\",\n",
    "                        returns=\"queries\",\n",
    "                        body_attr=\"text\",\n",
    "                        wmodel=wmodel,\n",
    "                        controls={\"qe\":\"off\"})\n",
    "\n",
    "rtr = textscorer.transform(df)\n",
    "\n",
    "method = 'TERRIER'\n",
    "tag = wmodel\n",
    "fn = f'{out_dir}/{method}-{wmodel}-dev.trec.txt'\n",
    "\n",
    "ptio._write_results_trec(rtr.query('rank < 5'), fn, run_name=wmodel)\n",
    "d = ptio._read_results_trec(fn)\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [P@5, R@5, MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:42:18.824 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.2947368421052632,\n",
       " 'R@5': 0.7210526315789473,\n",
       " 'AP': 0.6890350877192982}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import P, R, MAP\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "wmodel = 'DPH'\n",
    "\n",
    "textscorer = TextScorer(takes=\"docs\",\n",
    "                        returns=\"queries\",\n",
    "                        body_attr=\"text\",\n",
    "                        wmodel=wmodel,\n",
    "                        controls={\"qe\":\"off\"})\n",
    "\n",
    "rtr = textscorer.transform(df)\n",
    "\n",
    "method = 'TERRIER'\n",
    "tag = wmodel\n",
    "fn = f'{out_dir}/{method}-{wmodel}-dev.trec.txt'\n",
    "\n",
    "ptio._write_results_trec(rtr.query('rank < 5'), fn, run_name=wmodel)\n",
    "d = ptio._read_results_trec(fn)\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [P@5, R@5, MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qe on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:42:21.480 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n",
      "23:42:21.485 [main] WARN org.terrier.querying.QueryExpansion - qemodel control not set for QueryExpansion post process. Using default model Bo1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.2736842105263158,\n",
       " 'R@5': 0.7035087719298246,\n",
       " 'AP': 0.6442982456140351}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import P, R, MAP\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "wmodel = 'BM25'\n",
    "\n",
    "textscorer = TextScorer(takes=\"docs\",\n",
    "                        returns=\"queries\",\n",
    "                        body_attr=\"text\",\n",
    "                        wmodel=wmodel,\n",
    "                        controls={\"qe\":\"on\"})\n",
    "\n",
    "rtr = textscorer.transform(df)\n",
    "\n",
    "method = 'TERRIER'\n",
    "tag = wmodel\n",
    "fn = f'{out_dir}/{method}-{wmodel}-qe-dev.trec.txt'\n",
    "\n",
    "ptio._write_results_trec(rtr.query('rank < 5'), fn, run_name=wmodel)\n",
    "d = ptio._read_results_trec(fn)\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [P@5, R@5, MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:42:24.091 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n",
      "23:42:24.094 [main] WARN org.terrier.querying.QueryExpansion - qemodel control not set for QueryExpansion post process. Using default model Bo1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.2631578947368421, 'R@5': 0.6771929824561403, 'AP': 0.637719298245614}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import P, R, MAP\n",
    "\n",
    "from pyterrier.batchretrieve import TextScorer\n",
    "\n",
    "wmodel = 'DPH'\n",
    "\n",
    "textscorer = TextScorer(takes=\"docs\",\n",
    "                        returns=\"queries\",\n",
    "                        body_attr=\"text\",\n",
    "                        wmodel=wmodel,\n",
    "                        controls={\"qe\":\"on\"})\n",
    "\n",
    "rtr = textscorer.transform(df)\n",
    "\n",
    "method = 'TERRIER'\n",
    "tag = wmodel\n",
    "fn = f'{out_dir}/{method}-{wmodel}-qe-dev.trec.txt'\n",
    "\n",
    "ptio._write_results_trec(rtr.query('rank < 5'), fn, run_name=wmodel)\n",
    "d = ptio._read_results_trec(fn)\n",
    "\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "golden_labels_file = task5_dir + '/data/dev_qrels.txt'\n",
    "\n",
    "golden = ptio.read_qrels(golden_labels_file)\n",
    "eval= ptpipelines.Evaluate(d, golden , metrics = [P@5, R@5, MAP],perquery=False)\n",
    "eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_441e3\" style='display:inline'>\n",
       "  <caption>Recall @ 5</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_441e3_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_441e3_level0_col1\" class=\"col_heading level0 col1\" >R@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row0\" class=\"row_heading level0 row0\" >5</th>\n",
       "      <td id=\"T_441e3_row0_col0\" class=\"data row0 col0\" >openai</td>\n",
       "      <td id=\"T_441e3_row0_col1\" class=\"data row0 col1\" >0.745263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row1\" class=\"row_heading level0 row1\" >3</th>\n",
       "      <td id=\"T_441e3_row1_col0\" class=\"data row1 col0\" >terrier</td>\n",
       "      <td id=\"T_441e3_row1_col1\" class=\"data row1 col1\" >0.721053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row2\" class=\"row_heading level0 row2\" >4</th>\n",
       "      <td id=\"T_441e3_row2_col0\" class=\"data row2 col0\" >sbert</td>\n",
       "      <td id=\"T_441e3_row2_col1\" class=\"data row2 col1\" >0.710175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_441e3_row3_col0\" class=\"data row3 col0\" >lucence</td>\n",
       "      <td id=\"T_441e3_row3_col1\" class=\"data row3 col1\" >0.695088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row4\" class=\"row_heading level0 row4\" >2</th>\n",
       "      <td id=\"T_441e3_row4_col0\" class=\"data row4 col0\" >tfidf</td>\n",
       "      <td id=\"T_441e3_row4_col1\" class=\"data row4 col1\" >0.668772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441e3_level0_row5\" class=\"row_heading level0 row5\" >0</th>\n",
       "      <td id=\"T_441e3_row5_col0\" class=\"data row5 col0\" >baseline</td>\n",
       "      <td id=\"T_441e3_row5_col1\" class=\"data row5 col1\" >0.635789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d06cc\" style='display:inline'>\n",
       "  <caption>Mean Average Precision</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d06cc_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_d06cc_level0_col1\" class=\"col_heading level0 col1\" >MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
       "      <td id=\"T_d06cc_row0_col0\" class=\"data row0 col0\" >terrier</td>\n",
       "      <td id=\"T_d06cc_row0_col1\" class=\"data row0 col1\" >0.689035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row1\" class=\"row_heading level0 row1\" >5</th>\n",
       "      <td id=\"T_d06cc_row1_col0\" class=\"data row1 col0\" >openai</td>\n",
       "      <td id=\"T_d06cc_row1_col1\" class=\"data row1 col1\" >0.644842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row2\" class=\"row_heading level0 row2\" >4</th>\n",
       "      <td id=\"T_d06cc_row2_col0\" class=\"data row2 col0\" >sbert</td>\n",
       "      <td id=\"T_d06cc_row2_col1\" class=\"data row2 col1\" >0.643965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_d06cc_row3_col0\" class=\"data row3 col0\" >lucence</td>\n",
       "      <td id=\"T_d06cc_row3_col1\" class=\"data row3 col1\" >0.635556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row4\" class=\"row_heading level0 row4\" >2</th>\n",
       "      <td id=\"T_d06cc_row4_col0\" class=\"data row4 col0\" >tfidf</td>\n",
       "      <td id=\"T_d06cc_row4_col1\" class=\"data row4 col1\" >0.593158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06cc_level0_row5\" class=\"row_heading level0 row5\" >0</th>\n",
       "      <td id=\"T_d06cc_row5_col0\" class=\"data row5 col0\" >baseline</td>\n",
       "      <td id=\"T_d06cc_row5_col1\" class=\"data row5 col1\" >0.561228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from clef.utils.scoring import eval_run_retrieval\n",
    "from clef.utils.data_loading import clef_base_path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.core.display import display_html\n",
    "\n",
    "sample_submission_file = clef_base_path + '/submission_samples/KGAT_zeroShot_evidence_English_dev.txt'\n",
    "\n",
    "lucene_submission_file = f'{out_dir}/LUCENE-dev.trec.txt'\n",
    "tfidf_submission_file = f'{out_dir}/TFIDF-dev.trec.txt'\n",
    "terrier_submission_file = f'{out_dir}/TERRIER-DPH-dev.trec.txt'\n",
    "sbert_submission_file = f'{out_dir}/SBERT-dev.trec.txt'\n",
    "openai_submission_file = f'{out_dir}/OPENAI-dev.trec.txt'\n",
    "\n",
    "golden_labels_file = clef_base_path + '/data/dev_qrels.txt'\n",
    "\n",
    "time_now  = datetime.datetime.now().strftime('%d_%m_%Y_%H_%M_%S') \n",
    "out_file = f'{out_dir}/eval/RQ1-{time_now}.csv'\n",
    "\n",
    "eval_data = [\n",
    "    ['baseline',    *[v for v in eval_run_retrieval(sample_submission_file,  golden_labels_file).values()]],\n",
    "    ['lucence',     *[v for v in eval_run_retrieval(lucene_submission_file,  golden_labels_file).values()]],\n",
    "    ['tfidf',       *[v for v in eval_run_retrieval(tfidf_submission_file,   golden_labels_file).values()]],\n",
    "    ['terrier',     *[v for v in eval_run_retrieval(terrier_submission_file, golden_labels_file).values()]],\n",
    "    ['sbert',       *[v for v in eval_run_retrieval(sbert_submission_file,   golden_labels_file).values()]],\n",
    "    ['openai',      *[v for v in eval_run_retrieval(openai_submission_file,  golden_labels_file).values()]],\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.columns = ['method', 'R@5', 'MAP']\n",
    "df_r5  = eval_df[['method', 'R@5']].sort_values('R@5', axis=0, ascending=False)\n",
    "df_map = eval_df[['method', 'MAP']].sort_values('MAP', axis=0, ascending=False)\n",
    "\n",
    "df1_styler = df_r5.style.set_table_attributes(\"style='display:inline'\").set_caption('Recall @ 5')\n",
    "df2_styler = df_map.style.set_table_attributes(\"style='display:inline'\").set_caption('Mean Average Precision')\n",
    "\n",
    "eval_df.to_csv(out_file)\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

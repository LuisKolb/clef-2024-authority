{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "base_path = '../clef2024-checkthat-lab/task5'\n",
    "base_data_path = os.path.join(base_path, 'data')\n",
    "\n",
    "# os.listdir(base_data_path)\n",
    "train_jsons = []\n",
    "\n",
    "with open(os.path.join(base_data_path, 'English_train.json'), encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        train_jsons += [json.loads(line)]\n",
    "\n",
    "# len(train_jsons)        \n",
    "dev_jsons = []\n",
    "\n",
    "with open(os.path.join(base_data_path, 'English_dev.json'), encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        dev_jsons += [json.loads(line)]\n",
    "\n",
    "# len(dev_jsons)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the NLI pipeline with a pre-trained model\n",
    "# nli_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "nli_pipeline = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "\n",
    "def check_evidence_with_statement(evidence, statement):\n",
    "    # Define the candidate labels for NLI\n",
    "    # candidate_labels = [\"SUPPORTS\", \"REFUTES\"]\n",
    "    input_text = f\"{evidence} [SEP] {statement}\"\n",
    "\n",
    "    # Use the NLI pipeline to predict the relationship\n",
    "    # result = nli_pipeline(evidence, hypothesis=statement, candidate_labels=candidate_labels, multi_label=False)\n",
    "    result = nli_pipeline(input_text)\n",
    "\n",
    "    # Return the result\n",
    "    return result\n",
    "\n",
    "def create_json_obj(rumor_json):\n",
    "    if rumor_json['evidence']:\n",
    "        rumor_text = rumor_json['rumor']\n",
    "        # evidence_sentences = '. '.join([e[2] for e in rumor_json['evidence']])\n",
    "        # result = check_evidence_with_statement(evidence_sentences, rumor_text)\n",
    "\n",
    "        # # print(rumor_json)\n",
    "\n",
    "        # if result:\n",
    "        #     highest_score_label = result[\"labels\"][0]\n",
    "        #     highest_score = result[\"scores\"][0]\n",
    "        #     # print(f\"Label: {highest_score_label}, Score: {highest_score}\")\n",
    "        # else:\n",
    "        #     # print(\"Error in processing the NLI task.\")\n",
    "        #     pass\n",
    "        \n",
    "        label_map = {\n",
    "            \"CONTRADICTION\": \"REFUTES\",\n",
    "            \"NEUTRAL\": \"NOT ENOUGH INFO\",\n",
    "            \"ENTAILMENT\": \"SUPPORTS\"\n",
    "        }\n",
    "        \n",
    "        predicted_evidence = []\n",
    "        scores = []\n",
    "\n",
    "        for author_account, tweet_id, evidence_text in rumor_json['evidence']:\n",
    "            res = check_evidence_with_statement(evidence_text, rumor_text)\n",
    "            label = label_map[res[0]['label']]\n",
    "            score = res[0]['score']\n",
    "            # CLEF CheckThat! task 5: score is [-1, +1] where \n",
    "            #   -1 means evidence strongly refuted\n",
    "            #   +1 means evidence strongly supports\n",
    "            if label == \"REFUTES\":\n",
    "                score *= -1\n",
    "            elif label == \"NOT ENOUGH INFO\":\n",
    "                score = 0\n",
    "\n",
    "            predicted_evidence += [[\n",
    "                author_account,\n",
    "                tweet_id,\n",
    "                evidence_text,\n",
    "                score,\n",
    "            ]]\n",
    "\n",
    "            scores += [score]\n",
    "\n",
    "        cumsum = sum(scores) / len(scores)\n",
    "        \n",
    "        if cumsum > 0.3:\n",
    "            pred_label = \"SUPPORTS\"\n",
    "        elif cumsum < -0.3:\n",
    "            pred_label = \"REFUTES\"\n",
    "        else:\n",
    "            pred_label = \"NOT ENOUGH INFO\"\n",
    "\n",
    "        res_json = {\n",
    "            \"id\": rumor_json['id'],\n",
    "            \"predicted_label\": pred_label,\n",
    "            \"claim\": rumor_json['rumor'],\n",
    "            \"label\": rumor_json['label'],\n",
    "            \"predicted_evidence\": predicted_evidence,\n",
    "        }\n",
    "\n",
    "        return res_json\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:57<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import write_jsonlines_from_dicts\n",
    "from tqdm import tqdm\n",
    "\n",
    "fn = 'temp-data/zeroshot-ver.jsonl'\n",
    "res_dicts = []\n",
    "for item in tqdm(dev_jsons):\n",
    "    res = create_json_obj(item)\n",
    "    if res:\n",
    "        res_dicts += [res]\n",
    "\n",
    "write_jsonlines_from_dicts(fn, res_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from C/lef2024-checkthat-lab/task5/scorer/verification_scorer.py for ease of use during dev only\n",
    "import argparse\n",
    "import jsonlines\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def strict_f1(actual, predicted, actual_evidence, predicted_evidence, label):\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] != \"NOT ENOUGH INFO\":\n",
    "            if (actual[i] == label) & (\n",
    "                (predicted[i] == label)\n",
    "                & (bool(set(predicted_evidence[i]) & set(actual_evidence[i])) == True)\n",
    "            ):\n",
    "                tp = tp + 1\n",
    "            elif (actual[i] != label) & (predicted[i] == label):\n",
    "                fp = fp + 1\n",
    "            elif (actual[i] == label) & (\n",
    "                (predicted[i] == label)\n",
    "                & (bool(set(predicted_evidence[i]) & set(actual_evidence[i])) == False)\n",
    "            ):\n",
    "                fp = fp + 1\n",
    "            elif (predicted[i] != label) & (actual[i] == label):\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if (actual[i] == label) & (predicted[i] == label):\n",
    "                tp = tp + 1\n",
    "            elif (actual[i] != label) & (predicted[i] == label):\n",
    "                fp = fp + 1\n",
    "            elif (predicted[i] != label) & (actual[i] == label):\n",
    "                fn = fn + 1\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1(actual, predicted, label):\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(actual)):\n",
    "        if (actual[i] == label) & (predicted[i] == label):\n",
    "            tp = tp + 1\n",
    "        elif (actual[i] != label) & (predicted[i] == label):\n",
    "            fp = fp + 1\n",
    "        elif (predicted[i] != label) & (actual[i] == label):\n",
    "            fn = fn + 1\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1_macro(actual, predicted):\n",
    "    # `macro` f1- unweighted mean of f1 per label\n",
    "    return np.mean([f1(actual, predicted, label) for label in np.unique(actual)])\n",
    "\n",
    "\n",
    "def f1_macro_strict(actual, predicted, actual_evidence, predicted_evidence):\n",
    "    # `macro` f1- unweighted mean of macro-f1 per label\n",
    "    return np.mean(\n",
    "        [\n",
    "            strict_f1(actual, predicted, actual_evidence, predicted_evidence, label)\n",
    "            for label in np.unique(actual)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_run(pred_file, gold_file, out_file):\n",
    "\n",
    "    gold_dict_labels = {}\n",
    "    gold_dict_evidence = {}\n",
    "    for line in jsonlines.open(gold_file):\n",
    "        gold_dict_labels[line[\"id\"]] = line[\"label\"]\n",
    "        temp_ev = []\n",
    "        for ev in line[\"evidence\"]:\n",
    "            temp_ev.append(str(ev[1]))\n",
    "        gold_dict_evidence[line[\"id\"]] = temp_ev\n",
    "\n",
    "    pred = [line for line in jsonlines.open(pred_file)]\n",
    "    pred_labels = [line[\"predicted_label\"] for line in pred]\n",
    "    pred_evidence = []\n",
    "    for line in pred:\n",
    "        pred_instance = []\n",
    "        for ev in line[\"predicted_evidence\"]:\n",
    "            pred_instance.append(str(ev[1]))\n",
    "        pred_evidence.append(pred_instance)\n",
    "\n",
    "    actual_labels = []\n",
    "    actual_evidence = []\n",
    "    for line in pred:\n",
    "        actual_labels.append(gold_dict_labels[line[\"id\"]])\n",
    "        actual_instance = []\n",
    "        for i in gold_dict_evidence[line[\"id\"]]:\n",
    "            actual_instance.append(i)\n",
    "        actual_evidence.append(actual_instance)\n",
    "\n",
    "    # compute macro-F1 and strict macro-F1\n",
    "    macro_F1 = f1_macro(actual_labels, pred_labels)\n",
    "    strict_macro_F1 = f1_macro_strict(\n",
    "        actual_labels, pred_labels, actual_evidence, pred_evidence\n",
    "    )\n",
    "\n",
    "    print(\"Macro_F1\", macro_F1)\n",
    "    print(\"Strict Macro_F1\", strict_macro_F1)\n",
    "\n",
    "    result_list = [pred_file.split(\"/\")[-1], macro_F1, strict_macro_F1]\n",
    "    with open(out_file, \"a\") as f_object:\n",
    "        writer_object = writer(f_object, delimiter=\"\\t\")\n",
    "        writer_object.writerow(result_list)\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "Macro_F1 0.5081585081585082\n",
      "Strict Macro_F1 0.5081585081585082\n",
      "nli\n",
      "Macro_F1 0.5098039215686274\n",
      "Strict Macro_F1 0.5098039215686274\n"
     ]
    }
   ],
   "source": [
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "\n",
    "sample_submission_file = task5_dir + '/submission_samples/KGAT_zeroShot_verification_English_dev.json'\n",
    "\n",
    "nli_submission_file = 'temp-data/zeroshot-ver.jsonl'\n",
    "ground_truth_file = task5_dir + '/data/Arabic_dev.json'\n",
    "out_file = 'temp-data/out.csv'\n",
    "\n",
    "print('sample')\n",
    "eval_run(sample_submission_file,ground_truth_file, out_file)\n",
    "\n",
    "print('nli')\n",
    "eval_run(nli_submission_file,ground_truth_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 96 training json objects and 32 dev objects\n"
     ]
    }
   ],
   "source": [
    "from utils import load_rumors_from_jsonl\n",
    "import os\n",
    "\n",
    "out_dir = './temp-data'\n",
    "\n",
    "clef_path = '../clef2024-checkthat-lab/task5'\n",
    "data_path = os.path.join(clef_path, 'data')\n",
    "\n",
    "filepath_train = os.path.join(data_path, 'English_train.json')\n",
    "filepath_dev = os.path.join(data_path, 'English_dev.json')\n",
    "\n",
    "train_jsons = load_rumors_from_jsonl(filepath_train)\n",
    "dev_jsons = load_rumors_from_jsonl(filepath_dev)\n",
    "\n",
    "print(f'loaded {len(train_jsons)} training json objects and {len(dev_jsons)} dev objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "\n",
    "def retrieve_relevant_documents(query: str, timeline: List[List[str]]) -> List[Tuple[str, str, str, float]]:\n",
    "    # Get only doc texts\n",
    "    author_accounts = [t[0] for t in timeline]\n",
    "    tweet_ids = [t[1] for t in timeline]\n",
    "    documents = [t[2] for t in timeline]\n",
    "\n",
    "    # Combine query and documents for TF-IDF vectorization\n",
    "    combined_texts = [query] + documents\n",
    "    \n",
    "    # Generate TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "    \n",
    "    # Calculate similarity of the query to each document\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "    \n",
    "    # Rank documents based on similarity scores\n",
    "    ranked_doc_indices = similarity_scores.argsort()[0][::-1]\n",
    "\n",
    "    # Sort the documents according to rank\n",
    "    ranked_scores = [similarity_scores[0][i] for i in ranked_doc_indices]\n",
    "    ranked_authors = [author_accounts[i] for i in ranked_doc_indices]\n",
    "    ranked_ids = [tweet_ids[i] for i in ranked_doc_indices]\n",
    "    ranked_documents = [documents[i] for i in ranked_doc_indices]\n",
    "\n",
    "    # Create a list of tuples of shape (author, evidence_id, evidence_text, score)\n",
    "    ranked_tuples = (list(zip(ranked_authors, ranked_ids, ranked_documents, ranked_scores)))\n",
    "    \n",
    "    return ranked_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RankedDocs\n",
    "from typing import List\n",
    "\n",
    "def retrieve_using_tfidf(query: str, timeline: List[List[str]], k: int = 5) -> List[RankedDocs]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - query: a rumor or claim tweet text\n",
    "        - timeline: timeline surrounding the tweet\n",
    "        - k: top-k results to return. defaults to 5 \n",
    "\n",
    "    Returns: \n",
    "    List of tuples of shape [(author_account, authority_tweet_id, doc_text, rank, score), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    ranked_docs = retrieve_relevant_documents(query, timeline)\n",
    "\n",
    "    res = []\n",
    "    for rank, (author_account, authority_tweet_id, doc_text, score) in enumerate(ranked_docs[:k]):\n",
    "        res += [(author_account, authority_tweet_id, doc_text, rank+1, score)] \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from typing import Dict, TypedDict, Union\n",
    "\n",
    "class VerificationResult(TypedDict):\n",
    "    label: str\n",
    "    score: float\n",
    "\n",
    "# Initialize the NLI pipeline with a pre-trained model\n",
    "# nli_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "nli_pipeline = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "\n",
    "def check_statement_with_evidence(statement: str, evidence: str) -> VerificationResult:\n",
    "    # Define the candidate labels for NLI\n",
    "    # candidate_labels = [\"SUPPORTS\", \"REFUTES\"]\n",
    "    input_text = f\"{evidence} [SEP] {statement}\"\n",
    "\n",
    "    # Use the NLI pipeline to predict the relationship\n",
    "    # result = nli_pipeline(evidence, hypothesis=statement, candidate_labels=candidate_labels, multi_label=False)\n",
    "    result = nli_pipeline(input_text)\n",
    "\n",
    "    # Return the result\n",
    "    return result[0]\n",
    "\n",
    "def factcheck_using_evidence(claim: str, evidence: List[RankedDocs]):\n",
    "    \"\"\"\n",
    "    Predict a judgement for a rumor using the retrieved evidence.\n",
    "\n",
    "    Parameters:\n",
    "        - rumor_dict (Dict): a Python Dict of a single rumor from the dataset, extended by the key 'rerieved_evidence'\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "        \"CONTRADICTION\": \"REFUTES\",\n",
    "        \"NEUTRAL\": \"NOT ENOUGH INFO\",\n",
    "        \"ENTAILMENT\": \"SUPPORTS\"\n",
    "    }\n",
    "    \n",
    "    predicted_evidence = []\n",
    "    scores = []\n",
    "\n",
    "    for author_account, tweet_id, evidence_text, rank, score in evidence:\n",
    "        res = check_statement_with_evidence(claim, evidence_text)\n",
    "        label = label_map[res['label']]\n",
    "        score = res['score']\n",
    "\n",
    "        # CLEF CheckThat! task 5: score is [-1, +1] where \n",
    "        #   -1 means evidence strongly refuted\n",
    "        #   +1 means evidence strongly supports\n",
    "\n",
    "        if label == \"REFUTES\":\n",
    "            score *= -1\n",
    "        elif label == \"NOT ENOUGH INFO\":\n",
    "            score = 0 # TODO uhmmm...\n",
    "\n",
    "        predicted_evidence += [[\n",
    "            author_account,\n",
    "            tweet_id,\n",
    "            evidence_text,\n",
    "            score,\n",
    "        ]]\n",
    "\n",
    "        scores += [score]\n",
    "\n",
    "    cumsum = sum(scores) / len(scores)\n",
    "    \n",
    "    if cumsum > 0.3:\n",
    "        pred_label = \"SUPPORTS\"\n",
    "    elif cumsum < -0.3:\n",
    "        pred_label = \"REFUTES\"\n",
    "    else:\n",
    "        pred_label = \"NOT ENOUGH INFO\"\n",
    "\n",
    "    return pred_label, predicted_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running start-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:53<00:00,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import write_jsonlines_from_dicts\n",
    "from tqdm import tqdm\n",
    "\n",
    "ranked_docs_by_id = {}\n",
    "rumors_retrieved = []\n",
    "res_jsons = []\n",
    "\n",
    "class OutputDict(TypedDict):\n",
    "    id: str\n",
    "    label: str\n",
    "    claim: str\n",
    "    predicted_label: str\n",
    "    predicted_evidence: List[List[Union[str, float]]]\n",
    "\n",
    "for rumor_dict in tqdm(dev_jsons):\n",
    "\n",
    "    # unpack the dict from the dataset\n",
    "    rumor_id = rumor_dict['id']\n",
    "    query = rumor_dict['rumor']\n",
    "    label = rumor_dict['label']\n",
    "    timeline = rumor_dict['timeline']\n",
    "    evidence = rumor_dict['evidence']\n",
    "    \n",
    "    ranked_docs = retrieve_using_tfidf(query, timeline, 5)\n",
    "\n",
    "    pred_label, pred_evidence = factcheck_using_evidence(query, ranked_docs)\n",
    "\n",
    "    res_json = {\n",
    "        \"id\": rumor_id,\n",
    "        \"label\": label,\n",
    "        \"claim\": query,\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"predicted_evidence\": pred_evidence,\n",
    "    }\n",
    "    res_jsons += [res_json]\n",
    "\n",
    "outfile_tfidf_ver = 'temp-data/zeroshot-ver-from-tfidf.jsonl'\n",
    "write_jsonlines_from_dicts(outfile_tfidf_ver, res_jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "Macro_F1 0.5081585081585082\n",
      "Strict Macro_F1 0.5081585081585082\n",
      "nli\n",
      "Macro_F1 0.24111560953666214\n",
      "Strict Macro_F1 0.20797720797720798\n"
     ]
    }
   ],
   "source": [
    "from scoring_utils import eval_run\n",
    "\n",
    "task5_dir = '../clef2024-checkthat-lab/task5'\n",
    "\n",
    "sample_submission_file = task5_dir + '/submission_samples/KGAT_zeroShot_verification_English_dev.json'\n",
    "\n",
    "nli_submission_file = 'temp-data/zeroshot-ver-from-tfidf.jsonl'\n",
    "ground_truth_file = task5_dir + '/data/Arabic_dev.json'\n",
    "out_file = 'temp-data/out.csv'\n",
    "\n",
    "print('sample')\n",
    "eval_run(sample_submission_file,ground_truth_file, out_file)\n",
    "\n",
    "print('nli')\n",
    "eval_run(nli_submission_file,ground_truth_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

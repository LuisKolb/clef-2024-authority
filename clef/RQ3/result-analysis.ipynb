{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "root_path = '../../'\n",
    "out_dir = './data-out/setup2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clef.utils.scoring import eval_run_custom\n",
    "from clef.utils.data_loading import task5_dir\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.core.display import display_html\n",
    "\n",
    "\n",
    "sample_submission_file = f'{root_path}/{task5_dir}/submission_samples/KGAT_zeroShot_verification_English_dev.json'\n",
    "\n",
    "rq3_nli_submission_file = f'{out_dir}/zeroshot-ver-rq3-nli.jsonl'\n",
    "rq3_openai_submission_file = f'{out_dir}/zeroshot-ver-rq3-openai.jsonl'\n",
    "\n",
    "ground_truth_file = f'{root_path}/{task5_dir}/data/English_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def strict_f1(actual, predicted, actual_evidence, predicted_evidence, label):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] != \"NOT ENOUGH INFO\":\n",
    "            if (actual[i] == label) & (\n",
    "                (predicted[i] == label)\n",
    "                & (bool(set(predicted_evidence[i]) & set(actual_evidence[i])) == True)\n",
    "            ):\n",
    "                tp = tp + 1\n",
    "            elif (actual[i] != label) & (predicted[i] == label):\n",
    "                fp = fp + 1\n",
    "            elif (actual[i] == label) & (\n",
    "                (predicted[i] == label)\n",
    "                & (bool(set(predicted_evidence[i]) & set(actual_evidence[i])) == False)\n",
    "            ):\n",
    "                fp = fp + 1\n",
    "            elif (predicted[i] != label) & (actual[i] == label):\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if (actual[i] == label) & (predicted[i] == label):\n",
    "                tp = tp + 1\n",
    "            elif (actual[i] != label) & (predicted[i] == label):\n",
    "                fp = fp + 1\n",
    "            elif (predicted[i] != label) & (actual[i] == label):\n",
    "                fn = fn + 1\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1(actual, predicted, label):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(actual)):\n",
    "        if (actual[i] == label) & (predicted[i] == label):\n",
    "            tp = tp + 1\n",
    "        elif (actual[i] != label) & (predicted[i] == label):\n",
    "            fp = fp + 1\n",
    "        elif (predicted[i] != label) & (actual[i] == label):\n",
    "            fn = fn + 1\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1_macro(actual, predicted):\n",
    "    # `macro` f1- unweighted mean of f1 per label\n",
    "    for label in np.unique(actual):\n",
    "        pass        \n",
    "    return np.mean([f1(actual, predicted, label) for label in np.unique(actual)])\n",
    "\n",
    "\n",
    "def f1_macro_strict(actual, predicted, actual_evidence, predicted_evidence):\n",
    "    # `macro` f1- unweighted mean of macro-f1 per label\n",
    "    return np.mean(\n",
    "        [\n",
    "            strict_f1(actual, predicted, actual_evidence, predicted_evidence, label)\n",
    "            for label in np.unique(actual)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines, json\n",
    "\n",
    "def eval_run_custom(pred_file, gold_file, out_file):\n",
    "    \"\"\"\n",
    "    basically the same, but without saving to file\n",
    "    \"\"\"\n",
    "    gold_dict_labels = {}\n",
    "    gold_dict_evidence = {}\n",
    "    for line in jsonlines.open(gold_file):\n",
    "        gold_dict_labels[line[\"id\"]] = line[\"label\"]\n",
    "        temp_ev = []\n",
    "        for ev in line[\"evidence\"]:\n",
    "            temp_ev.append(str(ev[1]))\n",
    "        gold_dict_evidence[line[\"id\"]] = temp_ev\n",
    "    # print(json.dumps(gold_dict_evidence, indent=2))\n",
    "    # return (1,1)\n",
    "    pred = [line for line in jsonlines.open(pred_file)]\n",
    "    pred_labels = [line[\"predicted_label\"] for line in pred]\n",
    "    pred_evidence = []\n",
    "    for line in pred:\n",
    "        pred_instance = []\n",
    "        for ev in line[\"predicted_evidence\"]:\n",
    "            pred_instance.append(str(ev[1]))\n",
    "        pred_evidence.append(pred_instance)\n",
    "    # print(json.dumps(pred_evidence, indent=2))\n",
    "    # return (1,1)\n",
    "    actual_labels = []\n",
    "    actual_evidence = []\n",
    "    for line in pred:\n",
    "        actual_labels.append(gold_dict_labels[line[\"id\"]])\n",
    "        actual_instance = []\n",
    "        for i in gold_dict_evidence[line[\"id\"]]:\n",
    "            actual_instance.append(i)\n",
    "        actual_evidence.append(actual_instance)\n",
    "    # print(json.dumps(actual_labels, indent=2))\n",
    "    # print(json.dumps(actual_evidence, indent=2))\n",
    "    # return (1,1)\n",
    "        \n",
    "    # compute macro-F1 and strict macro-F1\n",
    "    macro_F1 = f1_macro(actual_labels, pred_labels)\n",
    "    strict_macro_F1 = f1_macro_strict(\n",
    "        actual_labels, pred_labels, actual_evidence, pred_evidence\n",
    "    )\n",
    "    return (macro_F1, strict_macro_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def get_actual_pred_labels(pred_file, gold_file):\n",
    "    gold_dict_labels = {}\n",
    "    for line in jsonlines.open(gold_file):\n",
    "        gold_dict_labels[line[\"id\"]] = line[\"label\"]\n",
    "\n",
    "    pred = [line for line in jsonlines.open(pred_file)]\n",
    "    pred_labels = [line[\"predicted_label\"] for line in pred]\n",
    "\n",
    "    actual_labels = []\n",
    "    for line in pred:\n",
    "        actual_labels.append(gold_dict_labels[line[\"id\"]])\n",
    "    \n",
    "    return (actual_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'NOT ENOUGH INFO'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'NOT ENOUGH INFO'),\n",
       " ('REFUTES', 'REFUTES'),\n",
       " ('REFUTES', 'NOT ENOUGH INFO'),\n",
       " ('REFUTES', 'NOT ENOUGH INFO'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('SUPPORTS', 'SUPPORTS'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO'),\n",
       " ('NOT ENOUGH INFO', 'NOT ENOUGH INFO')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "act, pred = get_actual_pred_labels(rq3_openai_submission_file, ground_truth_file)\n",
    "list(zip(act, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_pred_labels_evidence(pred_file, gold_file):\n",
    "    gold_dict_labels = {}\n",
    "    gold_dict_evidence = {}\n",
    "    for line in jsonlines.open(gold_file):\n",
    "        gold_dict_labels[line[\"id\"]] = line[\"label\"]\n",
    "        temp_ev = []\n",
    "        for ev in line[\"evidence\"]:\n",
    "            temp_ev.append(str(ev[1]))\n",
    "        gold_dict_evidence[line[\"id\"]] = temp_ev\n",
    "    pred = [line for line in jsonlines.open(pred_file)]\n",
    "    pred_labels = [line[\"predicted_label\"] for line in pred]\n",
    "    pred_evidence = []\n",
    "    for line in pred:\n",
    "        pred_instance = []\n",
    "        for ev in line[\"predicted_evidence\"]:\n",
    "            pred_instance.append(str(ev[1]))\n",
    "        pred_evidence.append(pred_instance)\n",
    "    actual_labels = []\n",
    "    actual_evidence = []\n",
    "    for line in pred:\n",
    "        actual_labels.append(gold_dict_labels[line[\"id\"]])\n",
    "        actual_instance = []\n",
    "        for i in gold_dict_evidence[line[\"id\"]]:\n",
    "            actual_instance.append(i)\n",
    "        actual_evidence.append(actual_instance)\n",
    "    return(actual_labels, pred_labels, actual_evidence, pred_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234715165767147523\n",
      "1233784722238705670\n",
      "1304111096949866497\n",
      "1341840863358554115\n",
      "1341782311889723401\n",
      "1341779925326581761\n",
      "1341429691417305091\n",
      "1340648347502342145\n",
      "1340641178920873985\n",
      "1340280471306027009\n",
      "1341324720164261888\n",
      "1339883857374539778\n",
      "1342093406395314178\n",
      "1342067208856489985\n",
      "1341764926612844546\n",
      "1341669834329903104\n",
      "1341504694603157506\n",
      "1341478274845839362\n",
      "1340680064506204162\n",
      "1340259651171065856\n",
      "1339948400154963970\n",
      "1339895685752135684\n",
      "1339878931177086976\n",
      "1436952831215476739\n",
      "1403716782276464643\n",
      "1403741413817438210\n",
      "1403795269855191040\n",
      "1608880491989700608\n"
     ]
    }
   ],
   "source": [
    "actual_labels, pred_labels, actual_evidence, pred_evidence = get_actual_pred_labels_evidence(rq3_openai_submission_file, ground_truth_file)\n",
    "for act, pred in list(zip(actual_evidence,pred_evidence)):\n",
    "    for a in act:\n",
    "        if a not in pred:\n",
    "            print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bf0f5\" style='display:inline'>\n",
       "  <caption>macro-F1</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bf0f5_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_bf0f5_level0_col1\" class=\"col_heading level0 col1\" >macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bf0f5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bf0f5_row0_col0\" class=\"data row0 col0\" >RQ3-openai</td>\n",
       "      <td id=\"T_bf0f5_row0_col1\" class=\"data row0 col1\" >0.894949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_20db7\" style='display:inline'>\n",
       "  <caption>strict-macro-F1</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_20db7_level0_col0\" class=\"col_heading level0 col0\" >method</th>\n",
       "      <th id=\"T_20db7_level0_col1\" class=\"col_heading level0 col1\" >strict-macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_20db7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_20db7_row0_col0\" class=\"data row0 col0\" >RQ3-openai</td>\n",
       "      <td id=\"T_20db7_row0_col1\" class=\"data row0 col1\" >0.894949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# basic score eval\n",
    "\n",
    "# time_now  = datetime.datetime.now().strftime('%d_%m_%Y_%H_%M_%S') \n",
    "# out_file = f'{out_dir}/eval/RQ3-{time_now}.csv'\n",
    "\n",
    "eval_data = [\n",
    "    # ['baseline',    *eval_run_custom(sample_submission_file, ground_truth_file, '')],\n",
    "    # ['RQ3-nli',     *eval_run_custom(rq3_nli_submission_file, ground_truth_file, '')],\n",
    "    ['RQ3-openai',  *eval_run_custom(rq3_openai_submission_file, ground_truth_file, '')],\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.columns = ['method', 'macro-F1', 'strict-macro-F1']\n",
    "\n",
    "df_r5  = eval_df[['method', 'macro-F1']].sort_values('macro-F1', axis=0, ascending=False)\n",
    "df_map = eval_df[['method', 'strict-macro-F1']].sort_values('strict-macro-F1', axis=0, ascending=False)\n",
    "\n",
    "df1_styler = df_r5.style.set_table_attributes(\"style='display:inline'\").set_caption('macro-F1')\n",
    "df2_styler = df_map.style.set_table_attributes(\"style='display:inline'\").set_caption('strict-macro-F1')\n",
    "\n",
    "# eval_df.to_csv(out_file)\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
